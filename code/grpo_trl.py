"""
æ­¥éª¤3: GRPOè®­ç»ƒ (ä¸­æ–‡å¹½é»˜ç”Ÿæˆ)

GRPOè®¾è®¡:
- Policy: SFTåçš„Qwen3-4B (å¯è®­ç»ƒ, 4-bit + LoRA)
- RM: è®­ç»ƒå¥½çš„DeBERTaæ‰“åˆ†æ¨¡å‹ (frozen, ç”¨äºè®¡ç®—reward)

Rewardæµç¨‹:
1. Qwen3ç”Ÿæˆ â†’ token ids
2. Actor tokenizerè§£ç  â†’ ä¸­æ–‡æ–‡æœ¬å­—ç¬¦ä¸²
3. DeBERTa tokenizerç¼–ç  â†’ DeBERTaçš„token ids
4. DeBERTaæ¨¡å‹æ‰“åˆ† â†’ rewardåˆ†æ•°
"""
import torch
import os
import shutil
import pandas as pd
from datasets import Dataset
from accelerate import PartialState
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    BitsAndBytesConfig,
    GenerationConfig
)
from trl import ModelConfig, get_peft_config, GRPOConfig, GRPOTrainer
from peft import PeftModel, LoraConfig, get_peft_model, prepare_model_for_kbit_training  # æ·»åŠ PEFTå¯¼å…¥

# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# ============ é…ç½® ============
# æ¨¡å‹è·¯å¾„
BASE_MODEL_PATH = 'unsloth/Qwen3-1.7B'  # åŸºç¡€æ¨¡å‹ (ä¸SFTè®­ç»ƒæ—¶ç›¸åŒ)
SFT_LORA_PATH = 'model/zh_actor_sft'  # SFTè®­ç»ƒçš„LoRAæƒé‡
RM_MODEL_PATH = '../../humor_ppo_qwen3-4b-true/code/new_code2.12/code_zh/RM/model/zh_reward_model_deberta'
RM_TOKENIZER_PATH = 'IDEA-CCNL/Erlangshen-DeBERTa-v2-97M-Chinese'

# å®šä¹‰åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„
MERGED_MODEL_DIR = "model/qwen3_1.7b_sft_merged"

# æ•°æ®è·¯å¾„
TRAIN_DATA_FILE = '../pre_data/zh_humor_with_prompts.csv'
TEST_DATA_FILE = '../../humor_ppo_qwen3-4b-true/pre_data/humor_only_with_prompts/zh_humor_with_prompts_test.csv'

# è¾“å‡ºè·¯å¾„
OUTPUT_DIR = 'model/zh_grpo_trl'

# GRPOè®­ç»ƒå‚æ•°
LEARNING_RATE = 1e-6
BATCH_SIZE = 16
GRADIENT_ACCUMULATION_STEPS = 1
EPOCHS = 3
LOGGING_STEPS = 10
SAVE_STEPS = 100

# GRPOç‰¹å®šå‚æ•°
NUM_SAMPLE_GENERATIONS = 4  # æ¯ä¸ªpromptç”Ÿæˆçš„æ ·æœ¬æ•°ï¼ˆç»„å¤§å°ï¼‰
generation_batch_size = 64  #ç”Ÿæˆæ‰¹æ¬¡å¤§å° ç­‰äºNUM_SAMPLE_GENERATIONS*batch_size
MAX_NEW_TOKENS = 128
TEMPERATURE = 0.8
TOP_P = 0.95

# LoRAé…ç½®
LORA_R = 16
LORA_ALPHA = 32
LORA_DROPOUT = 0.05

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# Promptæ¨¡æ¿
PROMPT_TEMPLATE_HEADLINE = """è¯·æ ¹æ®ä»¥ä¸‹æ–°é—»æ ‡é¢˜ï¼Œåˆ›ä½œä¸€æ®µå¹½é»˜çš„ä¸­æ–‡æ–‡æœ¬ï¼š

æ ‡é¢˜ï¼š{headline}

å¹½é»˜æ–‡æœ¬ï¼š"""

PROMPT_TEMPLATE_WORDS = """è¯·ä½¿ç”¨ä»¥ä¸‹ä¸¤ä¸ªè¯è¯­ï¼Œåˆ›ä½œä¸€æ®µå¹½é»˜çš„ä¸­æ–‡æ–‡æœ¬ï¼š

è¯è¯­ï¼š{word1}ã€{word2}

å¹½é»˜æ–‡æœ¬ï¼š"""

# æ¸…ç†è¾“å‡ºç›®å½•
shutil.rmtree(OUTPUT_DIR, ignore_errors=True)

print("=" * 80)
print("GRPOè®­ç»ƒ (ä¸­æ–‡å¹½é»˜ç”Ÿæˆ)")
print("=" * 80)
print(f"åŸºç¡€æ¨¡å‹: {BASE_MODEL_PATH}")
print(f"SFT LoRAæƒé‡: {SFT_LORA_PATH}")
print(f"RMæ¨¡å‹: {RM_MODEL_PATH}")
print(f"è®­ç»ƒæ•°æ®: {TRAIN_DATA_FILE}")
print(f"æµ‹è¯•æ•°æ®: {TEST_DATA_FILE}")
print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")


# ============ åŠ è½½æ•°æ® ============
print("\n" + "=" * 80)
print("æ­¥éª¤1: åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®")
print("=" * 80)

# åŠ è½½è®­ç»ƒæ•°æ®
train_df = pd.read_csv(TRAIN_DATA_FILE, encoding='utf-8')
print(f"è®­ç»ƒæ•°æ®æ–‡ä»¶: {TRAIN_DATA_FILE}")
print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_df)}")

#############################åªä½¿ç”¨å‰1000æ¡æ•°æ®
train_df = train_df.head(1000)
print(f" æˆªæ–­å®Œæˆï¼šåªä½¿ç”¨å‰ {len(train_df)} æ¡åŸå§‹æ•°æ®è¿›è¡Œè®­ç»ƒ")

# åŠ è½½æµ‹è¯•æ•°æ®
test_df = pd.read_csv(TEST_DATA_FILE, encoding='utf-8')
print(f"\næµ‹è¯•æ•°æ®æ–‡ä»¶: {TEST_DATA_FILE}")
print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_df)}")

#æµ‹è¯•é›†ä¹Ÿå»ºè®®æˆªæ–­ä¸€ä¸‹ï¼Œæ¯”å¦‚åªç”¨å‰ 100 æ¡ï¼Œé˜²æ­¢è¯„ä¼°é˜¶æ®µç­‰å¤ªä¹…
test_df = test_df.head(100)
print(f"âœ… æˆªæ–­å®Œæˆï¼šåªä½¿ç”¨å‰ {len(test_df)} æ¡æ•°æ®è¿›è¡Œæµ‹è¯•")

# æ˜¾ç¤ºè®­ç»ƒæ•°æ®ç¤ºä¾‹
print("\nè®­ç»ƒæ•°æ®ç¤ºä¾‹:")
for i in range(min(3, len(train_df))):
    row = train_df.iloc[i]
    print(f"\næ ·æœ¬ {i+1}:")
    print(f"  Headline: {row['headline']}")
    print(f"  Words: {row['word1']}, {row['word2']}")
    print(f"  Joke: {row['joke'][:50]}...")

# ============ æ„å»ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ® ============
print("\n" + "=" * 80)
print("æ­¥éª¤2: æ„å»ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®")
print("=" * 80)

# âš ï¸ å…³é”®ä¿®æ”¹: GRPOéœ€è¦åˆ—åä¸º'prompt'è€Œä¸æ˜¯'query'
# å¹¶ä¸”ä¸éœ€è¦é¢„å…ˆåˆ†è¯,ç›´æ¥ä½¿ç”¨æ–‡æœ¬
train_data = []
for idx, row in train_df.iterrows():
    headline = row['headline']
    word1 = row['word1']
    word2 = row['word2']
    
    # æ–¹å¼1: ä½¿ç”¨headline
    prompt_headline = PROMPT_TEMPLATE_HEADLINE.format(headline=headline)
    train_data.append({"prompt": prompt_headline})  # åˆ—åæ”¹ä¸º'prompt'
    
    # æ–¹å¼2: ä½¿ç”¨words
    prompt_words = PROMPT_TEMPLATE_WORDS.format(word1=word1, word2=word2)
    train_data.append({"prompt": prompt_words})  # åˆ—åæ”¹ä¸º'prompt'

print(f"è®­ç»ƒPromptæ€»æ•°: {len(train_data)}")
print(f"  - åŸºäºheadline: {len([q for i, q in enumerate(train_data) if i % 2 == 0])}")
print(f"  - åŸºäºwords: {len([q for i, q in enumerate(train_data) if i % 2 == 1])}")

# æ„å»ºæµ‹è¯•æ•°æ®
test_data = []
for idx, row in test_df.iterrows():
    headline = row['headline']
    word1 = row['word1']
    word2 = row['word2']
    
    # æ–¹å¼1: ä½¿ç”¨headline
    prompt_headline = PROMPT_TEMPLATE_HEADLINE.format(headline=headline)
    test_data.append({"prompt": prompt_headline})  # åˆ—åæ”¹ä¸º'prompt'
    
    # æ–¹å¼2: ä½¿ç”¨words
    prompt_words = PROMPT_TEMPLATE_WORDS.format(word1=word1, word2=word2)
    test_data.append({"prompt": prompt_words})  # åˆ—åæ”¹ä¸º'prompt'

print(f"\næµ‹è¯•Promptæ€»æ•°: {len(test_data)}")
print(f"  - åŸºäºheadline: {len([q for i, q in enumerate(test_data) if i % 2 == 0])}")
print(f"  - åŸºäºwords: {len([q for i, q in enumerate(test_data) if i % 2 == 1])}")

# æ˜¾ç¤ºè®­ç»ƒpromptç¤ºä¾‹
print("\nè®­ç»ƒPromptç¤ºä¾‹:")
for i in range(min(3, len(train_data))):
    print(f"\nç¤ºä¾‹ {i+1}:")
    print(train_data[i]['prompt'].strip())

# è½¬æ¢ä¸ºDataset - ç›´æ¥ä½¿ç”¨æ–‡æœ¬,ä¸è¿›è¡Œåˆ†è¯
train_dataset = Dataset.from_list(train_data)
eval_dataset = Dataset.from_list(test_data)

print(f"\nDatasetåˆ›å»ºå®Œæˆ:")
print(f"  - è®­ç»ƒé›†: {len(train_dataset)} æ¡ (æ¥è‡ªç‹¬ç«‹è®­ç»ƒæ•°æ®)")
print(f"  - è¯„ä¼°é›†: {len(eval_dataset)} æ¡ (æ¥è‡ªç‹¬ç«‹æµ‹è¯•æ•°æ®)")
print(f"  âš ï¸ æ³¨æ„: æ•°æ®é›†ä¿æŒæ–‡æœ¬æ ¼å¼,ä¸è¿›è¡Œé¢„åˆ†è¯")


# ============ åŠ è½½Tokenizer ============
print("\n" + "=" * 80)
print("æ­¥éª¤3: åŠ è½½Tokenizer")
print("=" * 80)

# é‡åŒ–é…ç½®
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Policy tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    BASE_MODEL_PATH,  # ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„tokenizer
    padding_side="left",
    trust_remote_code=True
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

print("Policyåˆ†è¯å™¨åŠ è½½å®Œæˆ")
print(f"  - Vocab size: {len(tokenizer)}")
print(f"  - Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
print(f"  - EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")

# RM tokenizer
rm_tokenizer = AutoTokenizer.from_pretrained(RM_TOKENIZER_PATH)
print("\nRMåˆ†è¯å™¨åŠ è½½å®Œæˆ")
print(f"  - Vocab size: {len(rm_tokenizer)}")


# ============ åŠ è½½Policyæ¨¡å‹ (åŸºç¡€æ¨¡å‹ + SFT LoRA) ============
print("\n" + "=" * 80)
print("æ­¥éª¤4: å‡†å¤‡Policyæ¨¡å‹ (ç¦»çº¿åˆå¹¶ SFT LoRA + 4bitåŠ è½½)")
print("=" * 80)

# 4.1 æ£€æŸ¥æ˜¯å¦å·²ç»åˆå¹¶è¿‡ã€‚å¦‚æœæ²¡æœ‰ï¼Œåˆ™ä»¥ bfloat16 (ä¸ä½¿ç”¨4bit) åŠ è½½å¹¶åˆå¹¶
if not os.path.exists(MERGED_MODEL_DIR):
    print(f"4.1 é¦–æ¬¡è¿è¡Œï¼šä»¥ bfloat16 åŠ è½½åŸºç¡€æ¨¡å‹è¿›è¡Œåˆå¹¶ (ä¸é‡åŒ–)")
    base_model_for_merge = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        torch_dtype=torch.bfloat16,
        device_map="cpu",  # ä½¿ç”¨ CPU åˆå¹¶ä»¥èŠ‚çœæ˜¾å­˜ï¼Œå¦‚æœæ˜¾å¡æ˜¾å­˜å……è£•ä¹Ÿå¯ä»¥ç”¨ "auto"
        trust_remote_code=True,
    )
    
    print(f"4.2 åŠ è½½SFT LoRAå¹¶åˆå¹¶: {SFT_LORA_PATH}")
    sft_model = PeftModel.from_pretrained(base_model_for_merge, SFT_LORA_PATH)
    merged_model = sft_model.merge_and_unload()
    
    print(f"4.3 ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {MERGED_MODEL_DIR}")
    merged_model.save_pretrained(MERGED_MODEL_DIR)
    tokenizer.save_pretrained(MERGED_MODEL_DIR)
    
    # å½»åº•æ¸…ç†å†…å­˜å’Œæ˜¾å­˜ï¼Œä¸ºåç»­çš„ 4bit GRPO è®­ç»ƒè…¾å‡ºç©ºé—´
    del base_model_for_merge, sft_model, merged_model
    import gc; gc.collect(); torch.cuda.empty_cache()
    print("  âœ“ åˆå¹¶å®Œæˆå¹¶å·²é‡Šæ”¾å†…å­˜ï¼")
else:
    print(f"4.1 å‘ç°å·²åˆå¹¶çš„æ¨¡å‹: {MERGED_MODEL_DIR}ï¼Œè·³è¿‡åˆå¹¶æ­¥éª¤ã€‚")

# 4.4 å°†åˆå¹¶å¥½çš„æ¨¡å‹å½“ä½œå…¨æ–°çš„"åŸºç¡€æ¨¡å‹"ï¼Œä»¥ 4bit æ–¹å¼åŠ è½½
print(f"\n4.4 ä»¥ 4bit é‡åŒ–æ–¹å¼åŠ è½½åˆå¹¶åçš„æ–°æ¨¡å‹ï¼Œå‡†å¤‡ GRPO è®­ç»ƒ")
policy = AutoModelForCausalLM.from_pretrained(
    MERGED_MODEL_DIR,
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16
)

# 4.5 å¼€å¯æ¢¯åº¦å¹¶æ‰‹åŠ¨æŒ‚è½½ GRPO ä¸“å± LoRA
print(f"\n4.5 å¼€å¯æ¢¯åº¦å¹¶æ‰‹åŠ¨æŒ‚è½½ GRPO ä¸“å± LoRA")

if hasattr(policy, "enable_input_require_grads"):
    policy.enable_input_require_grads()

policy.gradient_checkpointing_enable(gradient_checkpointing_kwargs={"use_reentrant": False})

grpo_lora_config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
    task_type="CAUSAL_LM"
)
policy = get_peft_model(policy, grpo_lora_config)

# ğŸš€ ç»ˆæä¿®å¤ 1ï¼šæ¨¡å—çº§åˆ«çš„ç±»å‹å¼ºè½¬ (æ¯”ä¿®æ”¹ param.data æ›´å½»åº•ï¼Œåªè½¬åŒ–éé‡åŒ–å±‚)
for name, module in policy.named_modules():
    # åŒ¹é…æ ‡å‡†çš„çº¿æ€§å±‚ (å¦‚ lm_head)ã€‚æ³¨æ„ï¼šè¿™ä¸ä¼šå½±å“ 4bit é‡åŒ–å±‚ (bnb.nn.Linear4bit)
    if isinstance(module, torch.nn.Linear):
        module.to(torch.bfloat16)
    # å°† LayerNorm å’Œ Embedding ä¹Ÿç»Ÿä¸€è½¬åŒ–
    if "norm" in name.lower() or "embed" in name.lower():
        module.to(torch.bfloat16)

# ğŸš€ ç»ˆæä¿®å¤ 2ï¼šçŒ´å­è¡¥ä¸ (Monkey Patch)ï¼Œç»™ generate æ–¹æ³•å¥—ä¸Šç¡¬ä»¶çº§çš„ BFloat16 å¼ºåˆ¶ä¸Šä¸‹æ–‡
original_generate = policy.generate
def autocast_generate(*args, **kwargs):
    # å¼ºåˆ¶è®©ç”Ÿæˆè¿‡ç¨‹åœ¨ BFloat16 çš„ Autocast ç¯å¢ƒä¸‹è¿è¡Œï¼Œæœç»ä¸€åˆ‡ç±»å‹ä¸åŒ¹é…ï¼
    with torch.autocast("cuda", dtype=torch.bfloat16):
        return original_generate(*args, **kwargs)
policy.generate = autocast_generate

print("  âœ“ Policyæ¨¡å‹åŠ è½½å®Œæˆ (åŸºç¡€æ¨¡å‹ + SFT + 4bité‡åŒ– + BFloat16æ¨¡å—çº§å¯¹é½ + Autocastä¿æŠ¤ + GRPO LoRA)")
policy.print_trainable_parameters()


# ============ åŠ è½½Reward Model ============
print("\n" + "=" * 80)
print("æ­¥éª¤5: åŠ è½½Reward Model")
print("=" * 80)

# åŠ è½½DeBERTaå¥–åŠ±æ¨¡å‹
deberta_rm = AutoModelForSequenceClassification.from_pretrained(
    RM_MODEL_PATH,
    device_map="auto",
    torch_dtype=torch.float16
)
deberta_rm.eval()
for param in deberta_rm.parameters():
    param.requires_grad = False

print("DeBERTa RMæ¨¡å‹åŠ è½½å®Œæˆ")

# âš ï¸ å…³é”®ä¿®æ”¹: GRPOä¸éœ€è¦é¢„å…ˆåˆ†è¯,ç›´æ¥ä½¿ç”¨æ–‡æœ¬æ•°æ®é›†
# åˆ é™¤äº†åŸæ¥çš„æ•°æ®é¢„å¤„ç†æ­¥éª¤

# ============ é…ç½®GRPOè®­ç»ƒå‚æ•° ============
print("\n" + "=" * 80)
print("æ­¥éª¤6: é…ç½®GRPOè®­ç»ƒå‚æ•°")
print("=" * 80)

training_args = GRPOConfig(
    output_dir=OUTPUT_DIR,
    learning_rate=LEARNING_RATE,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    num_train_epochs=EPOCHS,
    logging_steps=LOGGING_STEPS,
    save_steps=SAVE_STEPS,
    report_to="tensorboard",
    logging_dir=f"{OUTPUT_DIR}/logs",
    
    # GRPOæ ¸å¿ƒå‚æ•°
    num_generations=NUM_SAMPLE_GENERATIONS,  # ç»„å¤§å°G
    generation_batch_size=generation_batch_size,  # å¿…é¡»èƒ½è¢«num_generationsæ•´é™¤
    max_prompt_length=256,  # å¿…é¡»è®¾ç½®: Promptæœ€å¤§é•¿åº¦
    max_completion_length=MAX_NEW_TOKENS,  # å¿…é¡»è®¾ç½®: ç”Ÿæˆå†…å®¹æœ€å¤§é•¿åº¦
    
    # ç®—æ³•å‚æ•°
    beta=0.05,  # KLæ•£åº¦æƒ©ç½šç³»æ•°
    temperature=TEMPERATURE,
    
    # è¿è¡Œä¼˜åŒ–
    bf16=True,  # å¦‚æœç¡¬ä»¶æ”¯æŒ,å»ºè®®å¼€å¯
    remove_unused_columns=False,  # é‡è¦: é˜²æ­¢åˆ æ‰promptåˆ—
    
    # ä¼˜åŒ–å‚æ•°
    max_grad_norm=1.0,
    warmup_steps=50,
)

print("GRPOè®­ç»ƒå‚æ•°:")
print(f"  - å­¦ä¹ ç‡: {LEARNING_RATE}")
print(f"  - Batch size: {BATCH_SIZE}")
print(f"  - æ¢¯åº¦ç´¯ç§¯: {GRADIENT_ACCUMULATION_STEPS}")
print(f"  - è®­ç»ƒè½®æ•°: {EPOCHS}")
print(f"  - ç»„å¤§å° (num_generations): {NUM_SAMPLE_GENERATIONS}")
print(f"  - ç”Ÿæˆæ‰¹æ¬¡å¤§å° (generation_batch_size): {NUM_SAMPLE_GENERATIONS}")
print(f"  - Max prompt length: 256")
print(f"  - Max completion length: {MAX_NEW_TOKENS}")
print(f"  - Beta (KL penalty): 0.04")
print(f"  - æ¸©åº¦: {TEMPERATURE}")
print(f"  - BF16: True")
print(f"  - Remove unused columns: False")
print(f"  - TensorBoardæ—¥å¿—: {OUTPUT_DIR}/logs")

# ============ é…ç½®LoRA ============
print("\n" + "=" * 80)
print("æ­¥éª¤8: é…ç½®LoRA")
print("=" * 80)

model_args = ModelConfig(
    model_name_or_path=BASE_MODEL_PATH,  # ä½¿ç”¨åŸºç¡€æ¨¡å‹è·¯å¾„
    load_in_4bit=True,
    trust_remote_code=True,
    lora_r=LORA_R,
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    lora_target_modules=[
        "q_proj", "v_proj", "k_proj", "o_proj", 
        "gate_proj", "down_proj", "up_proj"
    ]
)

peft_config = get_peft_config(model_args)

print("LoRAé…ç½®:")
print(f"  - LoRA rank (r): {LORA_R}")
print(f"  - LoRA alpha: {LORA_ALPHA}")
print(f"  - LoRA dropout: {LORA_DROPOUT}")
print(f"  - ç›®æ ‡æ¨¡å—: {model_args.lora_target_modules}")

# ============ å®šä¹‰Reward Function ============
def reward_function(prompts, completions, **kwargs):
    """
    GRPOè¦æ±‚çš„reward functionæ ¼å¼
    
    Args:
        prompts: è¾“å…¥çš„promptåˆ—è¡¨
        completions: ç”Ÿæˆçš„completionåˆ—è¡¨
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        rewards: æ¯ä¸ªcompletionçš„rewardåˆ†æ•°åˆ—è¡¨
    """
    rewards = []
    
    for prompt, completion in zip(prompts, completions):
        # ç»„åˆprompt + completion
        full_text = prompt + completion
        
        # ä½¿ç”¨DeBERTa RMè¯„åˆ†
        rm_inputs = rm_tokenizer(
            full_text,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(deberta_rm.device)
        
        with torch.no_grad():
            outputs = deberta_rm(**rm_inputs)
            # å–å¹½é»˜logitä½œä¸ºreward
            reward = outputs.logits[0, 1].item()
        
        rewards.append(reward)
    
    return rewards

print("\nReward Functionå®šä¹‰å®Œæˆ")

# ============ åˆ›å»ºGRPOè®­ç»ƒå™¨ ============
print("\n" + "=" * 80)
print("æ­¥éª¤8: åˆ›å»ºGRPOè®­ç»ƒå™¨")
print("=" * 80)

trainer = GRPOTrainer(
    model=policy,  # ä¼ å…¥ä¸Šé¢å·²ç»æ‰‹åŠ¨æŒ‚è½½å¥½ LoRA çš„ PeftModel
    processing_class=tokenizer,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    reward_funcs=reward_function,
    # âš ï¸ å…³é”®ï¼šè¿™é‡Œå»æ‰äº† peft_config å‚æ•°ï¼Œé˜²æ­¢ TRL é‡å¤åŒ…è£…å¯¼è‡´æŠ¥é”™
)

print("GRPOè®­ç»ƒå™¨åˆ›å»ºå®Œæˆ")
print("  - æ¨¡å‹: åŸºç¡€æ¨¡å‹ + SFT (å·²åˆå¹¶) + é’ˆå¯¹ GRPO çš„æ–° LoRA")

# ============ å¼€å§‹è®­ç»ƒ ============
print("\n" + "=" * 80)
print("æ­¥éª¤9: å¼€å§‹GRPOè®­ç»ƒ")
print("=" * 80)
print("=" * 50)

trainer.train()

print("=" * 50)
print("GRPOè®­ç»ƒå®Œæˆ!")

# ============ ä¿å­˜æ¨¡å‹ ============
print("\n" + "=" * 80)
print("æ­¥éª¤10: ä¿å­˜æ¨¡å‹")
print("=" * 80)

os.makedirs(OUTPUT_DIR, exist_ok=True)
trainer.save_model(OUTPUT_DIR)

print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")


# ============ æµ‹è¯•ç”Ÿæˆ ============
print("\n" + "=" * 80)
print("æ­¥éª¤11: æµ‹è¯•ç”Ÿæˆ")
print("=" * 80)

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
trained_policy = AutoModelForCausalLM.from_pretrained(
    OUTPUT_DIR,
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True,
    dtype=torch.bfloat16
)
print("è®­ç»ƒå¥½çš„æ¨¡å‹åŠ è½½å®Œæˆ")

# é…ç½®ç”Ÿæˆå‚æ•°
generation_config = GenerationConfig(
    max_new_tokens=100,
    temperature=0.7,
    top_p=0.9,
    do_sample=True,
    pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id,
)

# æµ‹è¯•prompt
test_prompts = [
    PROMPT_TEMPLATE_HEADLINE.format(headline="å­¦ç”Ÿä¸Šè¯¾è¿Ÿåˆ°è¢«è€å¸ˆæ‰¹è¯„"),
    PROMPT_TEMPLATE_WORDS.format(word1="åƒ", word2="è”¬èœ"),
    PROMPT_TEMPLATE_HEADLINE.format(headline="ç¨‹åºå‘˜åŠ ç­åˆ°æ·±å¤œ"),
]

for i, prompt in enumerate(test_prompts):
    print(f"\n{'='*60}")
    print(f"æµ‹è¯• {i+1}:")
    print(f"{'='*60}")
    print(f"è¾“å…¥Prompt:")
    print(f"  {prompt.strip()}")
    
    inputs = tokenizer(prompt, return_tensors='pt').to(trained_policy.device)
    print(f"\nPrompt tokenæ•°: {inputs['input_ids'].shape[1]}")
    
    with torch.no_grad():
        outputs = trained_policy.generate(
            **inputs,
            generation_config=generation_config,
        )
    
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    generated_joke = generated_text[len(prompt):].strip()
    
    print(f"\nç”Ÿæˆçš„å¹½é»˜æ–‡æœ¬:")
    print(f"  {generated_joke}")
    print(f"\nç”Ÿæˆtokenæ•°: {outputs.shape[1] - inputs['input_ids'].shape[1]}")
    
    # ä½¿ç”¨RMè¯„åˆ†
    rm_inputs = rm_tokenizer(
        generated_text,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(deberta_rm.device)
    
    with torch.no_grad():
        rm_outputs = deberta_rm(**rm_inputs)
        logits = rm_outputs.logits
        probs = torch.softmax(logits, dim=-1)
        humor_score = probs[0, 1].item()
        humor_logit = logits[0, 1].item()
    
    print(f"\nRMè¯„åˆ†:")
    print(f"  ä¸å¹½é»˜æ¦‚ç‡: {probs[0, 0].item():.4f}")
    print(f"  å¹½é»˜æ¦‚ç‡: {humor_score:.4f}")
    print(f"  å¹½é»˜logit: {humor_logit:.4f}")
    print("-" * 60)

# ============ è®­ç»ƒæ€»ç»“ ============
print("\n" + "=" * 80)
print("è®­ç»ƒå®Œæˆæ€»ç»“")
print("=" * 80)
print(f"è®­ç»ƒæ•°æ®é‡: {len(train_dataset)}")
print(f"æ¯ä¸ªpromptç”Ÿæˆæ ·æœ¬æ•°: {NUM_SAMPLE_GENERATIONS}")
print(f"æ¨¡å‹ä¿å­˜ä½ç½®: {OUTPUT_DIR}")
print(f"TensorBoardæ—¥å¿—: {OUTPUT_DIR}/logs")
print(f"\næŸ¥çœ‹è®­ç»ƒæ›²çº¿:")
print(f"  tensorboard --logdir={OUTPUT_DIR}/logs")

print("\n" + "=" * 80)
print("GRPOè®­ç»ƒå®Œæˆï¼")
print("=" * 80)

